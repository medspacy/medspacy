{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "import medspacy\n",
    "from medspacy.preprocess import PreprocessingRule, Preprocessor\n",
    "from medspacy.ner import TargetRule\n",
    "from medspacy.context import ConTextRule\n",
    "from medspacy.section_detection import Sectionizer\n",
    "from medspacy.postprocess import PostprocessingRule, PostprocessingPattern, Postprocessor\n",
    "from medspacy.postprocess import postprocessing_functions\n",
    "from medspacy.visualization import visualize_ent, visualize_dep\n",
    "\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this notebook, we'll show how to use a pretrained model for target concept extraction instead of defining rules. We'll then add our additional components to show how medSpaCy can be used to combine statistical NLP with other rule-based components.\n",
    "\n",
    "As an example, we'll download the [med7](https://github.com/kormilitzin/med7) transformers model which can be used with spacy 3. This won't get all the concepts we're interested in, but will extract drug-related information like names and doses.\n",
    "\n",
    "We can install this model with `pip` using this GitHub link:\n",
    "```bash\n",
    "pip install https://med7.s3.eu-west-2.amazonaws.com/en_core_med7_trf.tar.gz\n",
    "```\n",
    "\n",
    "We'll also need to install `spacy-transformers`:\n",
    "```bash\n",
    "pip install spacy-transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://med7.s3.eu-west-2.amazonaws.com/en_core_med7_trf.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./discharge_summary.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model now can be loaded as any other spaCy model. We'll use `medspacy.load()` and pass in this model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = medspacy.load(\"en_core_med7_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process our text\n",
    "Similar to the last notebook, we'll add new rules to some of our components. Let's first look at what our model extracts out of the box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(nlp.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer = preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_rules = [\n",
    "    \n",
    "    PreprocessingRule(\n",
    "        re.compile(\"\\[\\*\\*[\\d]{1,4}-[\\d]{1,2}(-[\\d]{1,2})?\\*\\*\\]\"),\n",
    "        repl=\"01-01-2010\",\n",
    "        desc=\"Replace MIMIC date brackets with a generic date.\"\n",
    "    ),\n",
    "    \n",
    "    PreprocessingRule(\n",
    "        re.compile(\"\\[\\*\\*[\\d]{4}\\*\\*\\]\"),\n",
    "        repl=\"2010\",\n",
    "        desc=\"Replace MIMIC year brackets with a generic year.\"\n",
    "    ),\n",
    "    \n",
    "    PreprocessingRule(\n",
    "        re.compile(\"dx'd\"), repl=\"Diagnosed\", \n",
    "                  desc=\"Replace abbreviation\"\n",
    "    ),\n",
    "    \n",
    "    PreprocessingRule(\n",
    "        re.compile(\"tx'd\"), repl=\"Treated\", \n",
    "                  desc=\"Replace abbreviation\"\n",
    "    ),\n",
    "    \n",
    "        PreprocessingRule(\n",
    "        re.compile(\"\\[\\*\\*[^\\]]+\\]\"), \n",
    "        desc=\"Remove all other bracketed placeholder text from MIMIC\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.add(preprocess_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Rules\n",
    "The trained NER will add some new concepts that we weren't getting before, but we can customize with rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rules = [\n",
    "    TargetRule(literal=\"abdominal pain\", category=\"PROBLEM\"),\n",
    "    TargetRule(\"stroke\", \"PROBLEM\"),\n",
    "    TargetRule(\"hemicolectomy\", \"TREATMENT\"),\n",
    "    TargetRule(\"Hydrochlorothiazide\", \"TREATMENT\"),\n",
    "    TargetRule(\"colon cancer\", \"PROBLEM\"),\n",
    "    TargetRule(\"radiotherapy\", \"PROBLEM\",\n",
    "              pattern=[{\"LOWER\": \"xrt\"}]),\n",
    "    TargetRule(\"metastasis\", \"PROBLEM\"),\n",
    "    \n",
    "    TargetRule(\"Type II Diabetes Mellitus\", \"PROBLEM\", \n",
    "              pattern=[\n",
    "                  {\"LOWER\": \"type\"},\n",
    "                  {\"LOWER\": {\"IN\": [\"2\", \"ii\", \"two\"]}},\n",
    "                  {\"LOWER\": {\"IN\": [\"dm\", \"diabetes\"]}},\n",
    "                  {\"LOWER\": \"mellitus\", \"OP\": \"?\"}\n",
    "              ],),\n",
    "    TargetRule(\"Hypertension\", \"PROBLEM\",\n",
    "              pattern=[{\"LOWER\": {\"IN\": [\"htn\", \"hypertension\"]}}],),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher = nlp.get_pipe(\"medspacy_target_matcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher.add(target_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = nlp.get_pipe(\"medspacy_context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_rules = [\n",
    "    ConTextRule(\"diagnosed in <YEAR>\", \"HISTORICAL\", \n",
    "               pattern=[\n",
    "                   {\"LOWER\": \"diagnosed\"},\n",
    "                   {\"LOWER\": \"in\"},\n",
    "                   {\"LOWER\": {\"REGEX\": \"^[\\d]{4}$\"}}\n",
    "               ])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.add(context_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectionizer = nlp.add_pipe(\"medspacy_sectionizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.section_detection import SectionRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_rule = SectionRule(\"Brief Hospital Course:\", \"hospital-course\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectionizer.add(section_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "Here, we'll show another example of how postprocessing can be used. The NER component extracts **\"married\"** as a **\"TREATMENT\"** entity. While some might agree with this in a philosophical sense, it doesn't match our clinical definition very well. This shows a challenge of statistical NLP: we have relatively little control over what concepts are extracted by our model. But we can use some postprocessing rules to clean this up.\n",
    "\n",
    "Postprocessing can be used to remove or clean up entities which we know are incorrect. In this example, we'll just remove any entity where the text is **\"married\"**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessor = nlp.add_pipe(\"medspacy_postprocessor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_rules = [\n",
    "    PostprocessingRule(\n",
    "        patterns=[\n",
    "            PostprocessingPattern(condition=lambda ent: ent.text.lower() == \"married\"),\n",
    "        ],\n",
    "        action=postprocessing_functions.remove_ent,\n",
    "        description=\"Remove a specific misclassified span of text.\"\n",
    "    ),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessor.add(postprocess_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process our document\n",
    "Now, let's process the text with our complete pipeline and show the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text = \"Colon cancer dx'd in [**2554**], tx'd with hemicolectomy, chemo\"\n",
    "short_doc = nlp(short_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ent(short_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dep(short_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
