{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In these notebooks, we'll process an example clinical document with medSpaCy. First, we'll perform preprocessing and sentence segmentation. Next, we'll extract entities using rules, assert attributes such as negation and which section the entity occured in. We'll then put all of our pieces together to process the entire document. Finally, we'll look at an alternative pipeline using a pre-trained statistical model to extract target entities rather than rules.\n",
    "\n",
    "In this first notebook, we'll introduce the medSpaCy library and show how to load a medSpaCy pipeline. Then in the following notebooks we'll walk through each of the pipeline steps in more detail and apply a fully built pipeline on clinical text.\n",
    "\n",
    "These notebooks will give a high-level overview of each component, but the individual packages will typically contain more complete examples and documentation. \n",
    "\n",
    "**Disclaimer**: many of the subpackages are in beta, just like medSpaCy!\n",
    "\n",
    "# Notebooks\n",
    "\n",
    "## High-Level Notebooks\n",
    "The notebooks in this root directory will show an overview of medspaCy, how to load a basic pipeline, and the basics of how to use each component. Notebooks #1-3 will show how to use the components loaded in a default medSpaCy model. We'll then show how to add additional medSpaCy components such as section detection and pre/postprocessing. Then we'll show how a full medSpaCy pipeline processes example clinical text, first using custom rules and then using a pre-trained NER model. The final notebook shows how to use the 3 different visualizers.\n",
    "\n",
    "- [1-Introduction](1-Introduction.ipynb)\n",
    "- [2-Tokenizing-and-Sentence-Splitting](2-Tokenizing-and-Sentence-Splitting.ipynb)\n",
    "- [3-Information-Extraction](3-Information-Extraction.ipynb)\n",
    "- [4-Adding-Pipeline-Components](4-Adding-Pipeline-Components.ipynb)\n",
    "- [5-Full-Pipeline](5-Full-Pipeline.ipynb)\n",
    "- [6-Using-Pretrained-Models](6-Using-Pretrained-Models.ipynb)\n",
    "- [7-Visualizers](7-Visualizers.ipynb)\n",
    "- [8-Preprocessing-Postprocessing](8-Preprocessing-Postprocessing.ipynb)\n",
    "\n",
    "## Detailed Component Notebooks\n",
    "More detailed notebooks are provided for two of the components: `context` and `section_detection`. These will show more advanced functionality and detailed examples:\n",
    "- `./context/`\n",
    "- `./section_detection`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a medSpaCy model\n",
    "A medSpaCy model consists of a **base spaCy model** with **medSpaCy components added** to the pipeline. There are two primary ways that we can create a medSpaCy model:\n",
    "\n",
    "1. Load a full pipeline using `medspacy.load()`\n",
    "2. Add specific components to an existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load a full medSpaCy pipeline\n",
    "We can load a complete pipeline using the `medspacy.load()` function. By default, this will build off of spaCy's **en_core_web_sm** model and will include:\n",
    "- `Tokenizer`: A spaCy tokenizer with custom rules for handling clinical text\n",
    "- `Sentencizer`: A sentence splitter based on [PyRuSH](https://github.com/jianlins/PyRuSH)\n",
    "- `TargetMatcher` for extended rule-based matching\n",
    "- `ConText` for contextual analysis and attribute detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medspacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = medspacy.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also load from an existing model to add medspaCy pipeline components to your current pipeline. To do this, either pass in the model directly or the name of the model and any other components you want to enable/disable from the original model. For example, in the examples below we can load spaCy's `\"en_core_web_sm\"` model and disable the `\"ner\"` component so that we have POS tagger and dependency parser (which can be useful may not work too well with clinical text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp2 = spacy.load(\"en_core_web_sm\", disable={\"ner\"})\n",
    "nlp2 = medspacy.load(nlp2)\n",
    "nlp2.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = medspacy.load(\"en_core_web_sm\", disable={\"ner\"})\n",
    "nlp2.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default rules\n",
    "When available, components added by `medspacy.load()` include default rules. `Context`, and `sectionizer` will both contain default rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = nlp.get_pipe(\"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.item_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also set `load_rules` to `False` so that the components are all blank (other than PyRuSH, which requires rules to be instantiated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using specific models\n",
    "If you have other models installed, either in English or other languages, you can load that model in using the `model` argument. For example, to load a [sciSpaCy model](https://allenai.github.io/scispacy/) and use it with medSpaCy, first download the model:\n",
    "\n",
    "```bash\n",
    "pip install scispacy\n",
    "pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz\n",
    "```\n",
    "and then load it with medSpaCy:\n",
    "\n",
    "```python\n",
    "nlp = medspacy.load(\"en_core_sci_sm\", load_rules=False, disable=[\"target_matcher\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying components\n",
    "You can define which specific components to include or specific components to exclude through the `enable` and `disable` arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only load the default components\n",
    "nlp_default = medspacy.load()\n",
    "nlp_default.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All medspaCy components\n",
    "nlp_full = medspacy.load(enable=\"all\")\n",
    "print(nlp_full.pipe_names)\n",
    "print(nlp_full.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only load the target_matcher and custom tokenizer\n",
    "nlp_matcher_only = medspacy.load(enable=[\"tokenizer\", \"target_matcher\"])\n",
    "nlp_matcher_only.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable the custom tokenizer and context\n",
    "nlp_no_tok_context = medspacy.load(disable=[\"tokenizer\", \"context\"])\n",
    "nlp_no_tok_context.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert nlp_no_tok_context.tokenizer != nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add specific components to an existing model\n",
    "You can also import specific classes from medSpaCy, instantiate them yourself, and add them to an existing model. We'll show more examples of how to do this in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.context import ConTextComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ConTextComponent(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en.add_pipe(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Data\n",
    "For data, we will use this example text derived from the [MIMIC-II](https://mimic.physionet.org/) critical care dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./discharge_summary.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
